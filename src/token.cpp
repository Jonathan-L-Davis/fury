#include "token.h"
#include <fstream>
#include <iostream>

std::vector<token> tokenize( const std::string &file_name ){

    std::vector<token> retMe;

    std::fstream file(file_name,std::ios::in);

    std::string line;
    int line_depth = 0;
    int line_no = 1;

    auto next_token = [&line,&line_depth]{
        if(line == "")//avoid null string issues
            return std::string("");

        if( std::isspace(line[line_depth]) ){//advance until first non-ws
            while( (unsigned)line_depth < line.length() && std::isspace(line[line_depth]) )
                line_depth++;
        }

        int start_depth = line_depth;

        if( (unsigned)line_depth >= line.length() )//ignore ws that terminates a line
            return std::string("");

        //grab all non-ws chars
        while( (unsigned)line_depth < line.length() && !std::isspace(line[line_depth]) )
            line_depth++;

        return line.substr(start_depth,line_depth-start_depth);
    };

    while( std::getline( file, line) ){//line by line
        for( line_depth = 0; (unsigned)line_depth < line.length();  ){//tokenize only via ws breaks
            token tok = {next_token(),(unsigned)line_no};
            if(tok.text != "" )//don't push empty tokens, last token generated by a given line is a 'null' token
                retMe.push_back(tok);
        }
        line_no++;
    }

    return retMe;
}